{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input,Embedding,LSTM,Dense,GlobalMaxPooling1D,Flatten\n",
    "from  tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile content.json\n",
    "{\"intents\": [\n",
    "    {\"tag\": \"greeting\",\n",
    "     \"patterns\": [\"Hi\", \"Hey\", \"Is anyone there?\", \"Hello\", \"Good day\"],\n",
    "     \"responses\": [\"Hello, thanks for visiting KIIT university chatbot \", \"Good to see you again friend\", \"Hi there, how can I help?\"],\n",
    "     \"context_set\": \"\"\n",
    "    },\n",
    "    {\"tag\": \"goodbye\",\n",
    "     \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\"],\n",
    "     \"responses\": [\"See you later, thanks for visiting KIIT university chatbot\", \"Have a nice day and keep hustling\", \"Bye! Come back again soon.\"]\n",
    "    },\n",
    "    {\"tag\": \"thanks\",\n",
    "     \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\"],\n",
    "     \"responses\": [\"Happy to help students always!\", \"Any time!\", \"My pleasure\"]\n",
    "    },\n",
    "    {\"tag\": \"Fests / Cultural events\",\n",
    "     \"patterns\": [\"What are the fests organized in the University?\", \"What are the societies?\", \"What are the Cultural fests here?\" ],\n",
    "     \"responses\": [\"We have various clubs and societies like dance,singing,quiz,coding all registered under KSAC,KIIT fest is the largest organized fest of eastern india \"]\n",
    "    },\n",
    "    {\"tag\": \"location\",\n",
    "     \"patterns\": [\"What is the univerity location?\", \"Where is KIIT located?\", \"What is your address?\", \"Where is the campus?\" ],\n",
    "     \"responses\": [\"We have our one and only campus at Bhubaneswar,India with a massive area of 200 acres \", \"We have our one and only campus at Bhubaneswar,India with a massive area of 25 sq KM\"]\n",
    "    },\n",
    "    {\"tag\": \"Computersciencedepartment\",\n",
    "     \"patterns\": [\"How is the department of Computer Science?\", \"How is the computer science course in KIIT?\", \"Does KIIT provide the CS course in B.tech?\" ],\n",
    "     \"responses\": [\"We have a reowned world class CS Faculty with some eminent teaching infrastructure.\"]\n",
    "    },\n",
    "    {\"tag\": \"sports\",\n",
    "     \"patterns\": [\"What is the sports infrastructure like in KIIT?\", \"Does KIIT provide sports facility to students?\"],\n",
    "     \"responses\": [\"KIIT provide excellent sports facility , for a small example we have 8 gyms accross all the campuses and multiple swimming pools\", \"KIIT is a pioneer in the field of sports , for a small example we have 8 gyms accross all the campuses and multiple swimming pools\"]\n",
    "    },\n",
    "    {\"tag\": \"placements\",\n",
    "     \"patterns\": [\"What the placement scenario at KIIT?\", \"What type of companies visit KIIT during placements?\"],\n",
    "     \"responses\": [\"Our students crack top-knotch companies in placements, companies come in 3 rounds Day-0,Day-1 and Day-2.The highest offer was made by Yugabite for 53 LPA\"],\n",
    "     \"context_set\": \"jobs\"\n",
    "    },\n",
    "    {\"tag\": \"Exams\",\n",
    "     \"patterns\": [\"What is the exam structure like?\", \"What is the grading ctriteria?\"],\n",
    "     \"responses\": [\"We have a efficient grading system which helps in proper academic evaluation of all students.\"],\n",
    "     \"context_filter\": \"exam\"\n",
    "    },\n",
    "    {\"tag\": \"hostel\",\n",
    "     \"patterns\": [\"Is hostel accomodation available for students?\", \"What type of hostels are available?\"],\n",
    "     \"responses\": [\"All type of hostels including AC,Non-AC .We have all type of rooms including 3-bed,2-bed with attached bathrooms\"],\n",
    "     \"context_filter\": \"hostel\"\n",
    "    },\n",
    "    {\n",
    "      \"tag\":\"number1\",\n",
    "      \"patterns\":[\"61279 42001\"],\n",
    "      \"responses\": [\"200011\"]\n",
    "    },\n",
    "    {\n",
    "      \"tag\":\"number2\",\n",
    "      \"patterns\":[\"61279 42002\"],\n",
    "      \"responses\": [\"200012\"]\n",
    "    },\n",
    "    {\n",
    "      \"tag\":\"number3\",\n",
    "      \"patterns\":[\"73211 67811\"],\n",
    "      \"responses\": [\"200015\"]\n",
    "\n",
    "    },\n",
    "    {\n",
    "      \"tag\":\"number4\",\n",
    "      \"patterns\":[\"73211 67812\"],\n",
    "      \"responses\": [\"200016\"]\n",
    "\n",
    "    }\n",
    "\n",
    "]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "with open('content.json') as content:\n",
    "  data1=json.load(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all data to lists\n",
    "tags=[]\n",
    "inputs=[]\n",
    "responses={}\n",
    "for intent in data1['intents']:\n",
    "  responses[intent['tag']]=intent['responses']\n",
    "  for lines in intent['patterns']:\n",
    "    inputs.append(lines)\n",
    "    tags.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to dataframe\n",
    "data=pd.DataFrame({\"inputs\":inputs,\"tags\":tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctuations\n",
    "import string\n",
    "data['inputs']=data['inputs'].apply(lambda wrd:[lrts.lower() for lrts in wrd if lrts not in string.punctuation])\n",
    "data['inputs']=data['inputs'].apply(lambda wrd: ''.join(wrd))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the data\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(data['inputs'])\n",
    "train=tokenizer.texts_to_sequences(data['inputs'])\n",
    "#apply padding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "x_train=pad_sequences(train)\n",
    "\n",
    "#encoding the outputs\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "y_train=le.fit_transform(data['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=x_train.shape[1]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vocabulary\n",
    "vocabulary=len(tokenizer.word_index)\n",
    "print(\"no of unique words :\",vocabulary)\n",
    "output_length=le.classes_.shape[0]\n",
    "print(\"output length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model\n",
    "\n",
    "i=Input(shape=(input_shape))\n",
    "x=Embedding(vocabulary+1,10)(i)\n",
    "x=LSTM(10,return_sequences=True)(x)\n",
    "x=Flatten()(x)\n",
    "x=Dense(output_length,activation=\"softmax\")(x)\n",
    "model=Model(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "train=model.fit(x_train,y_train,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting model accuracy\n",
    "plt.plot(train.history['acc'],label='training set accuracy')\n",
    "plt.plot(train.history['loss'],label='training set loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatting\n",
    "import random\n",
    "\n",
    "while True:\n",
    "  texts_p=[]\n",
    "  prediction_input=input('You : ')\n",
    "  \n",
    "  #removing punctuation and converting to upper case\n",
    "  prediction_input=[letters.lower() for letters in prediction_input if letters not in string.punctuation]\n",
    "  prediction_input=''.join(prediction_input)\n",
    "  texts_p.append(prediction_input)\n",
    "\n",
    "  #tokenizing and padding\n",
    "  prediction_input=tokenizer.texts_to_sequences(texts_p)\n",
    "  prediction_input=np.array(prediction_input).reshape(-1)\n",
    "  prediction_input=pad_sequences([prediction_input],input_shape)\n",
    "\n",
    "  #getting output from model\n",
    "  output=model.predict( prediction_input)\n",
    "  output=output.argmax()\n",
    "\n",
    "  #finding the right tag and predicting\n",
    "  response_tag=le.inverse_transform([output])[0]\n",
    "  print(\"KIIT Help Bot : \",random.choice(responses[response_tag]))\n",
    "  if (response_tag == \"goodbye\") :\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='chatbot_trained.sav'\n",
    "pickle.dump(model,open(filename,'wb'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
